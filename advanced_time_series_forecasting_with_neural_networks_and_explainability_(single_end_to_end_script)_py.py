# -*- coding: utf-8 -*-
"""Advanced Time Series Forecasting with Neural Networks and Explainability (Single End-to-End Script).py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eOVA6mEQRteFoK-3F3pGTpvC2uLU86t
"""



# ============================================================
# Advanced Time Series Forecasting with Neural Networks
# and Explainability (Single End-to-End Script)
# ============================================================

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.arima.model import ARIMA
import shap
import warnings

warnings.filterwarnings("ignore")
torch.manual_seed(42)
np.random.seed(42)

# ------------------------------------------------------------
# 1. Generate Multivariate Time Series Data
# ------------------------------------------------------------
def generate_multivariate_series(n_steps=1500, n_features=5):
    t = np.arange(n_steps)
    data = []

    for i in range(n_features):
        signal = (
            np.sin(0.02 * t + i)
            + 0.5 * np.sin(0.05 * t)
            + np.random.normal(scale=0.3, size=n_steps)
        )
        data.append(signal)

    data = np.stack(data, axis=1)
    columns = [f"feat_{i}" for i in range(n_features)]
    return pd.DataFrame(data, columns=columns)


# ------------------------------------------------------------
# 2. Sequence Creation
# ------------------------------------------------------------
def create_sequences(df, target_col, lookback):
    X, y = [], []
    for i in range(len(df) - lookback):
        X.append(df.iloc[i:i + lookback].values)
        y.append(df.iloc[i + lookback][target_col])
    return np.array(X), np.array(y)


# ------------------------------------------------------------
# 3. LSTM Forecasting Model
# ------------------------------------------------------------
class LSTMModel(nn.Module):
    def __init__(self, n_features, hidden_dim):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


# ------------------------------------------------------------
# 4. Train and Evaluate LSTM (Walk-Forward Style)
# ------------------------------------------------------------
def train_lstm(X_train, y_train, X_test, y_test,
               hidden_dim=64, epochs=15, lr=0.001):

    model = LSTMModel(X_train.shape[2], hidden_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    train_ds = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32)
    )
    loader = DataLoader(train_ds, batch_size=32, shuffle=False)

    model.train()
    for _ in range(epochs):
        for xb, yb in loader:
            optimizer.zero_grad()
            preds = model(xb).squeeze()
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()

    model.eval()
    with torch.no_grad():
        preds = model(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()

    return preds, model


# ------------------------------------------------------------
# 5. ARIMA Baseline Model
# ------------------------------------------------------------
def arima_baseline(series, train_size):
    train = series[:train_size]
    test = series[train_size:]

    model = ARIMA(train, order=(2, 1, 2))
    fitted = model.fit()
    forecast = fitted.forecast(steps=len(test))

    return forecast


# ------------------------------------------------------------
# 6. Evaluation Metrics
# ------------------------------------------------------------
def evaluate(y_true, y_pred):
    return {
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "MAE": mean_absolute_error(y_true, y_pred)
    }


# ------------------------------------------------------------
# 7. Explainable AI using SHAP (Sequence-Aware)
# ------------------------------------------------------------
def explain_with_shap(model, X_background, X_explain):
    def model_wrapper(x):
        x = x.reshape(-1, X_explain.shape[1], X_explain.shape[2])
        with torch.no_grad():
            return model(torch.tensor(x, dtype=torch.float32)).numpy()

    background = X_background.reshape(X_background.shape[0], -1)
    explain = X_explain.reshape(X_explain.shape[0], -1)

    explainer = shap.KernelExplainer(model_wrapper, background)
    shap_values = explainer.shap_values(explain, nsamples=50)

    return shap_values


# ------------------------------------------------------------
# 8. Main Execution
# ------------------------------------------------------------
if __name__ == "__main__":

    # Generate dataset
    df = generate_multivariate_series()
    target_col = "feat_0"

    # Scale data
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(
        scaler.fit_transform(df),
        columns=df.columns
    )

    # Create sequences
    lookback = 30
    X, y = create_sequences(df_scaled, target_col, lookback)

    # Walk-forward split
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    # Train LSTM
    lstm_preds, lstm_model = train_lstm(
        X_train, y_train, X_test, y_test
    )

    lstm_metrics = evaluate(y_test, lstm_preds)

    # ARIMA baseline
    arima_preds = arima_baseline(df[target_col].values, split + lookback)
    arima_metrics = evaluate(
        df[target_col].values[split + lookback:], arima_preds
    )

    # SHAP Explainability
    shap_values = explain_with_shap(
        lstm_model,
        X_train[:50],
        X_test[:10]
    )

    # Results
    print("\nLSTM Metrics:", lstm_metrics)
    print("ARIMA Metrics:", arima_metrics)
    print("\nSHAP values computed for feature and temporal importance.")